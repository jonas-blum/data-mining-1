{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be546c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sepal Length:  -5.705507692307693\n",
      "Standard Deviation Sepal Length:  303.7889483450795\n"
     ]
    }
   ],
   "source": [
    "# TASK 1: reading the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "data = pd.read_csv(\"iris_data.csv\", delimiter=\";\")\n",
    "labels = pd.read_csv(\"iris_labels.csv\", delimiter=\";\")\n",
    "\n",
    "data = pd.merge(data , labels , on=\"id\", how=\"inner\")\n",
    "data.drop([\"examiner\"], axis=1, inplace=True)\n",
    "data = data.sort_values(\"species\")\n",
    "\n",
    "\n",
    "\n",
    "# What are the average length of sepals (sl) and their standard deviation?\n",
    "print(\"Average Sepal Length: \", data[\"sl\"].mean())\n",
    "print(\"Standard Deviation Sepal Length: \", data[\"sl\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0252f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species\n",
      "Iris-setosa        3000\n",
      "Iris-virginica     3000\n",
      "Iris-versicolor     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TASK 2: database preprocessing\n",
    "\n",
    "# How many instances are there for each class?\n",
    "print(data[\"species\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26f85987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the system doesn't know which values are missing, the missing values are interpreted as valid values which might lead to wrong results.\n",
      "Average Sepal Length:  3.5275947028025865\n",
      "Standard Deviation Sepal Length:  2.102492233385377\n",
      "Shape of data before removing outliers:  (6494, 6)\n",
      "Average Sepal Length:  3.5206258671188535\n",
      "Standard Deviation Sepal Length:  2.0185052102580663\n",
      "Shape of data after removing outliers:  (6487, 6)\n",
      "Outlier values: \n",
      "        pl   pw    sl    sw\n",
      "5960  5.8  4.5   1.5   0.4\n",
      "4624  5.7  4.5   1.5   0.5\n",
      "1095  5.7  4.5   1.3   0.5\n",
      "1428  5.9  2.7  51.0   1.5\n",
      "141   5.1  1.8   3.4   1.0\n",
      "752   6.4  2.9   5.8  23.0\n",
      "878   6.8  3.1   5.4  22.0\n"
     ]
    }
   ],
   "source": [
    "# TASK 3: data cleaning\n",
    "\n",
    "# Why is it important to let the system know which values are missing?\n",
    "print(\"If the system doesn't know which values are missing, the missing values are interpreted as valid values which might lead to wrong results.\")\n",
    "\n",
    "data = data[data[\"pl\"] != -9999]\n",
    "data = data[data[\"pw\"] != -9999]\n",
    "data = data[data[\"sl\"] != -9999]\n",
    "data = data[data[\"sw\"] != -9999]\n",
    "\n",
    "# 3.1 What are the average length of sepals (sl) and their standard deviation after declaring missing values\n",
    "print(\"Average Sepal Length: \", data[\"sl\"].mean())\n",
    "print(\"Standard Deviation Sepal Length: \", data[\"sl\"].std())\n",
    "\n",
    "# 3.2 What are the average length of sepals (sl) and their standard deviation after removing outliers\n",
    "print(\"Shape of data before removing outliers: \", data.shape)\n",
    "numerical_features = data[['pl', 'pw', 'sl', 'sw']]\n",
    "threshold = 3\n",
    "z_scores = np.abs(stats.zscore(numerical_features))\n",
    "data = data[(z_scores < threshold).all(axis=1)]\n",
    "print(\"Average Sepal Length: \", data[\"sl\"].mean())\n",
    "print(\"Standard Deviation Sepal Length: \", data[\"sl\"].std())\n",
    "print(\"Shape of data after removing outliers: \", data.shape)\n",
    "outlier_values = numerical_features[(z_scores >= threshold).any(axis=1)]\n",
    "print(\"Outlier values: \\n\", outlier_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7544c380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalized Sepal Length Mean:  0.05433455583272086\n",
      "Min-Max Normalized Sepal Length Standard Deviation:  0.04187909058872281\n",
      "Standardized Sepal Length Mean:  2.1007731023396865e-16\n",
      "Standardized Sepal Length Standard Deviation:  1.0\n",
      "[[-2.5415313   0.19744875 -0.01499683  0.08797746]\n",
      " [-2.59897427 -0.32060579 -0.04178496 -0.28297024]\n",
      " [-2.75198157 -0.14284286  0.10554679 -0.16772068]\n",
      " ...\n",
      " [ 2.43688895 -0.25481236  0.25970566 -0.07117731]\n",
      " [ 3.0027377   0.59545206  0.08196667  0.2031846 ]\n",
      " [ 2.03380798 -0.19624631 -0.0095211  -0.17893956]]\n",
      "Explained variance per feature:\n",
      "[0.91329008 0.04017737 0.03231569 0.01421686]\n",
      "So to retain at least 95% of the variance we need the first retain the first two principal components: 0.91329008 + 0.04017737 = 0.95346745 = 95.34%\n",
      "Variance captured by the first two components:  0.9534674471361828\n",
      "First principal component as a combination of the original attributes: \n",
      " [ 0.33847865 -0.0766534   0.86707949  0.35739281]\n",
      "Explained variance per feature for modified dataset:\n",
      "[9.99796380e-01 1.65483443e-04 2.45682155e-05 1.35679977e-05]\n",
      "Variance captured by the first component for the modified dataset:  0.9997963803439168\n",
      "Explained variance per feature for modified dataset with outlier:\n",
      "[9.98563202e-01 1.34471058e-03 5.39268273e-05 3.81602571e-05]\n",
      "Variance captured by the first component for the modified dataset with outlier:  0.9985632023314325\n"
     ]
    }
   ],
   "source": [
    "# TASK 4: data transformation\n",
    "\n",
    "# What are the average length and standard deviation of sepals after min-max normalization?\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaled = MinMaxScaler().fit_transform(numerical_features)\n",
    "print(\"Min-Max Normalized Sepal Length Mean: \", minmax_scaled[:,2].mean())\n",
    "print(\"Min-Max Normalized Sepal Length Standard Deviation: \", minmax_scaled[:,2].std())\n",
    "\n",
    "# What are the average length and standard deviation of sepals after standardization?\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sd_scaled = StandardScaler().fit_transform(numerical_features)\n",
    "print(\"Standardized Sepal Length Mean: \", sd_scaled[:,2].mean())\n",
    "print(\"Standardized Sepal Length Standard Deviation: \", sd_scaled[:,2].std())\n",
    "\n",
    "# How many components have been selected after 4.3?\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(numerical_features)\n",
    "print(principal_components)\n",
    "print(\"Explained variance per feature:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"So to retain at least 95% of the variance we need the first retain the first two principal components: 0.91329008 + 0.04017737 = 0.95346745 = 95.34%\")\n",
    "\n",
    "# How much variance is captured by the first two components?\n",
    "print(\"Variance captured by the first two components: \", pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1])\n",
    "\n",
    "# How is the first component defined as a combination of the original attributes?\n",
    "print(\"First principal component as a combination of the original attributes: \\n\", pca.components_[0])\n",
    "\n",
    "# How many components would have been selected after 4.4 (that is, with an attribute expressed on a larger range)?\n",
    "\n",
    "modified_dataset = numerical_features.copy()\n",
    "modified_dataset['pl'] = modified_dataset['pl'] * 100.0\n",
    "modified_principal_components = pca.fit_transform(modified_dataset)\n",
    "print(\"Explained variance per feature for modified dataset:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Variance captured by the first component for the modified dataset: \", pca.explained_variance_ratio_[0])\n",
    "\n",
    "# How many components would have been selected after 4.5 (that is, with an outlier)?\n",
    "modified_dataset = numerical_features.copy()\n",
    "modified_dataset.at[0, \"pl\" ] = 5000.0\n",
    "modified_principal_components = pca.fit_transform(modified_dataset)\n",
    "print(\"Explained variance per feature for modified dataset with outlier:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Variance captured by the first component for the modified dataset with outlier: \", pca.explained_variance_ratio_[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7755e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 5.1:\n",
      "(150, 6)\n",
      "species\n",
      "Iris-virginica     65\n",
      "Iris-setosa        64\n",
      "Iris-versicolor    21\n",
      "Name: count, dtype: int64\n",
      "Unique samples:  150\n",
      "\n",
      "Task 5.2:\n",
      "(150, 6)\n",
      "species\n",
      "Iris-setosa        69\n",
      "Iris-virginica     69\n",
      "Iris-versicolor    12\n",
      "Name: count, dtype: int64\n",
      "Unique samples:  150\n",
      "\n",
      "Task 5.3:\n",
      "(3243, 6)\n",
      "species\n",
      "Iris-virginica     1498\n",
      "Iris-setosa        1496\n",
      "Iris-versicolor     249\n",
      "Name: count, dtype: int64\n",
      "Unique samples:  3243\n",
      "\n",
      "Task 5.4:\n",
      "(150, 6)\n",
      "species\n",
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "Name: count, dtype: int64\n",
      "Unique samples:  150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/3sh8mbdx6p30vgwkwwz5p0vh0000gp/T/ipykernel_31126/668443839.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample3 = data.groupby(\"species\", group_keys=False).apply(\n",
      "/var/folders/w7/3sh8mbdx6p30vgwkwwz5p0vh0000gp/T/ipykernel_31126/668443839.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample4 = data.groupby(\"species\", group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# TASK 5:\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# 5.1\n",
    "print(\"\\nTask 5.1:\")\n",
    "sample1 = data.sample(n=150, random_state=random_state)\n",
    "print(sample1.shape)\n",
    "print(sample1[\"species\"].value_counts())\n",
    "print(\"Unique samples: \", sample1[\"id\"].nunique())\n",
    "\n",
    "# 5.2\n",
    "print(\"\\nTask 5.2:\")\n",
    "sample2 = data.sample(n=150, random_state=random_state, replace=True)\n",
    "print(sample2.shape)\n",
    "print(sample2[\"species\"].value_counts())\n",
    "print(\"Unique samples: \",sample2[\"id\"].nunique())\n",
    "\n",
    "# 5.3\n",
    "print(\"\\nTask 5.3:\")\n",
    "sample3 = data.groupby(\"species\", group_keys=False).apply(\n",
    "lambda x: x.sample(frac=0.5, random_state=random_state))\n",
    "print(sample3.shape)\n",
    "print(sample3[\"species\"].value_counts())\n",
    "print(\"Unique samples: \",sample3[\"id\"].nunique())\n",
    "\n",
    "# 5.4\n",
    "print(\"\\nTask 5.4:\")\n",
    "sample4 = data.groupby(\"species\", group_keys=False).apply(\n",
    "lambda x: x.sample(50, random_state=random_state))\n",
    "print(sample4.shape)\n",
    "print(sample4[\"species\"].value_counts())\n",
    "print(\"Unique samples: \",sample4[\"id\"].nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
